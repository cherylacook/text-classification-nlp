{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for Handwritten Digit Classification\n",
    "\n",
    "Objective: Implement a multi-layer perceptron (MLP) to classify handwritten digits from the scikit-learn Digits dataset. Evaluate the effect of different activation functions on convergence and test performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "The dataset is loaded from scikit-learn and preprocessed by:\n",
    "- Normalising the input features\n",
    "- Splitting into training and test sets (80-20)\n",
    "- Wrapping the data in PyTorch DataLoaders for batching.\n",
    "\n",
    "This ensures the MLP receives data in a format suitable for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6573,
     "status": "ok",
     "timestamp": 1716933264679,
     "user": {
      "displayName": "Cheryl C",
      "userId": "07083983319276283397"
     },
     "user_tz": -720
    },
    "id": "aKAxsm3DtLcE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "\n",
    "# Load the Digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "\n",
    "# Split the data into training and test sets (80%-20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=231)\n",
    "\n",
    "# Normalise the images by scaling pixel values to 0-1 range\n",
    "normaliser = MinMaxScaler()\n",
    "# Normaliser is fitted to the training data\n",
    "X_train_normalised = normaliser.fit_transform(X_train) \n",
    "# Normaliser fit to training set applied to test set\n",
    "X_test_normalised = normaliser.transform(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Converting NumPy arrays into PyTorch tensors to allow for multi-dimensional data rep. & efficient computation\n",
    "Using .float() for features since NN computations are more precise with floats.\n",
    "\"\"\"\n",
    "X_train_tensor = torch.from_numpy(X_train_normalised).float()\n",
    "# .long() for target values, since Pytorch loss expects 'long' format\n",
    "y_train_tensor = torch.from_numpy(y_train).long()\n",
    "X_test_tensor = torch.from_numpy(X_test_normalised).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "# Create DataLoader objects for both training and test sets\n",
    "\"\"\"\n",
    "Batch size 64 provides enough variability to aid generalisation,\n",
    "and enough samples to calculate stable gradient estimates for smoother training.\n",
    "\"\"\"\n",
    "batch_size = 64\n",
    "\n",
    "# TensorDataset() bundles multiple tensors into a single dataset\n",
    "training_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# DataLoader divides the dataset into smaller batches to allow for more frequent weight updates (once per batch).\n",
    "# For training data, shuffle=true to improve generalisation by preventing the model from learning artifical order-related data patterns.\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# For test data, shuffle=false to prevent introduction of randomness that isn't reflective of how the model would be used in real-world scenarios.\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Model Architecture\n",
    "\n",
    "A simple multilayer perceptron is implemented with an input layer, one hidden layer, and an output layer.\n",
    "The forward pass uses the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1716933270101,
     "user": {
      "displayName": "Cheryl C",
      "userId": "07083983319276283397"
     },
     "user_tz": -720
    },
    "id": "v32tazy4xyMN"
   },
   "outputs": [],
   "source": [
    "# MLP Model Implementation\n",
    "\n",
    "\"\"\"\n",
    "Defining a neural network class by extending torch.nn.Module.\n",
    "NN has one input layer with 64 neurons (since Digits dataset is 8x8 images), one hidden layer of 128 neurons, \n",
    "and one output layer with 10 neurons (since there are ten possible digit options).\n",
    "Using ReLU activation for the hidden layer, and the softmax activation for the output layer.\n",
    "\"\"\"\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# NeuralNetwork inherits from torch.nn.Module\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    # super() calls the torch.nn.Module constructor\n",
    "    super().__init__()\n",
    "    # Define the three layers using torch.nn.Linear (creates fully connected/linear layers)\n",
    "    # Input layer has 64 neurons (64 features) and transforms that input data into 128 neurons\n",
    "    self.input_layer = torch.nn.Linear(64, 128)\n",
    "    # Hidden layer with 128 neurons in and out\n",
    "    self.hidden_layer = torch.nn.Linear(128, 128)\n",
    "    # Output layer with 10 neurons out that correspond to the 10 classes (digits 0-9)\n",
    "    self.output_layer = torch.nn.Linear(128, 10)\n",
    "\n",
    "  # Forward pass through the neural network\n",
    "  def forward(self, x):\n",
    "    # Input data x undergoes linear transformation to z (z = w * x + b) and then ReLU activation is applied to preserve just positive values.\n",
    "    x = F.relu(self.input_layer(x))\n",
    "    # Same linear transformation and ReLU activation applied to hidden layer.\n",
    "    x = F.relu(self.hidden_layer(x))\n",
    "    # Softmax activation applied to output layer to convert raw output scores into probabilities for each possible output class.\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    x = softmax(self.output_layer(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training \n",
    "\n",
    "The MLP is trained using CrossEntropyLoss and the Adam optimiser for 15 epochs, with the training loss and accuracy printed per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5945,
     "status": "ok",
     "timestamp": 1716933281362,
     "user": {
      "displayName": "Cheryl C",
      "userId": "07083983319276283397"
     },
     "user_tz": -720
    },
    "id": "HRlITv9r8l98",
    "outputId": "da5e9e4c-9ee6-47d6-9399-ecba2a52527e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output:\n",
      "Epoch 1: Loss = 2.288, Accuracy = 41%\n",
      "Epoch 2: Loss = 2.166, Accuracy = 59%\n",
      "Epoch 3: Loss = 1.888, Accuracy = 66%\n",
      "Epoch 4: Loss = 1.733, Accuracy = 80%\n",
      "Epoch 5: Loss = 1.664, Accuracy = 84%\n",
      "Epoch 6: Loss = 1.631, Accuracy = 86%\n",
      "Epoch 7: Loss = 1.580, Accuracy = 93%\n",
      "Epoch 8: Loss = 1.559, Accuracy = 94%\n",
      "Epoch 9: Loss = 1.544, Accuracy = 94%\n",
      "Epoch 10: Loss = 1.531, Accuracy = 95%\n",
      "Epoch 11: Loss = 1.522, Accuracy = 96%\n",
      "Epoch 12: Loss = 1.518, Accuracy = 96%\n",
      "Epoch 13: Loss = 1.510, Accuracy = 97%\n",
      "Epoch 14: Loss = 1.512, Accuracy = 96%\n",
      "Epoch 15: Loss = 1.502, Accuracy = 97%\n"
     ]
    }
   ],
   "source": [
    "# MLP Model Training \n",
    "\n",
    "# Hyperparameter tuning: Chose CrossEntropyLoss as loss function, decided on Adam optimiser over SGD, and set an appropriate learning rate. \n",
    "# Trained the model for 15 epochs, printing the training loss and accuracy with each epoch.\n",
    "\n",
    "def train_show(network, lossFunc, optimiser, epochs):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      # Storing every calculated loss and accuracy in the epoch to calculate average\n",
    "      lossHistory = []\n",
    "      accuracyHistory = []\n",
    "      \n",
    "      network.train() # Set model to training mode, which has active dropout to make the model more robust and prevent overfitting\n",
    "        \n",
    "      for data, targ in training_loader:\n",
    "        optimiser.zero_grad() # PyTorch gradients accumulate by default, so they need to be zeroed out before each forward pass to avoid mixing between batches.\n",
    "\n",
    "        y = network.forward(data) # Perform forward pass by calling forward() in NeuralNetwork class\n",
    "        \n",
    "        loss = lossFunc(y,targ) # Calculate the loss\n",
    "        loss.backward() # Runs autograd to get the gradients needed by the optimiser: Computes gradients of the loss with respect to all model parameters using backpropagation.\n",
    "\n",
    "        optimiser.step() # Takes a step: updates the model parameters in the direction that minimises the loss using the calculated gradients.\n",
    "\n",
    "        \"\"\"\n",
    "        torch.argmax(y,dim=1) returns the index of the class with the highest\n",
    "        probability for each input sample in the batch, which is then compared\n",
    "        to the actual (targ) class value for that sample. A boolean tensor is\n",
    "        returned to indicate whether the prediction is correct or not,\n",
    "        and .float() converts that boolean tensor into a float tensor\n",
    "        (1 for correct, 0 for incorrect).\n",
    "        torch.mean() calculates the mean of all the input samples' float tensors\n",
    "        to effectively get the proportion of correct predictions for that batch.\n",
    "        \"\"\"\n",
    "        accuracy = torch.mean((torch.argmax(y,dim=1) == targ).float())\n",
    "          \n",
    "        # Add the loss and accuracy values to their lists. These lists will later be used to calculate the average over the epoch.\n",
    "        lossHistory.append(loss.detach().item()) # Extracting the loss value as a Python scalar.\n",
    "        accuracyHistory.append(accuracy.detach()) # Detaches the accuracy tensor from the computation graph\n",
    "\n",
    "      # Calculate average loss and accuracy for the current epoch\n",
    "      avg_loss = sum(lossHistory) / len(lossHistory)\n",
    "      avg_accuracy = sum(accuracyHistory) / len(accuracyHistory)\n",
    "\n",
    "      # Print average training loss and accuracy over the epoch\n",
    "      print(f\"Epoch {epoch+1}: Loss = {avg_loss:.3f}, Accuracy = {int(avg_accuracy*100)}%\")\n",
    "\n",
    "# Instantiating the neural network using the previously defined NeuralNetwork class\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Defining the loss function\n",
    "lossFunction = torch.nn.CrossEntropyLoss() # calculates how off predictions are from actual values\n",
    "\n",
    "# Choosing an optimiser and an appropriate learning rate: chose Adam over SGD since Adam has adaptive learning rate for each parameter\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training Output:\")\n",
    "train_show(model, lossFunction, optimiser, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "The trained model is evaluated on the test set, with the final test accuracy being reported to provide an unbiased estimate of its generalisation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1716933285671,
     "user": {
      "displayName": "Cheryl C",
      "userId": "07083983319276283397"
     },
     "user_tz": -720
    },
    "id": "lJ8xRLJr8oKz",
    "outputId": "eb4a5cf2-bb67-4dd9-e454-2d4d27d8b260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95%\n",
      "Test Image 1: Predicted Label = 5, Actual Label = 5\n",
      "Test Image 2: Predicted Label = 3, Actual Label = 3\n",
      "Test Image 3: Predicted Label = 1, Actual Label = 1\n",
      "Test Image 4: Predicted Label = 2, Actual Label = 2\n",
      "Test Image 5: Predicted Label = 9, Actual Label = 9\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation on Test Set\n",
    "\n",
    "# Measuring the model's accuracy on the test set, and printing five example predictions with their actual labels.\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "\n",
    "def test_network(network):\n",
    "  network.eval() # Set the network to evaluation mode, which disables dropout and normalises data using running mean and variance estimates collected during model training\n",
    "  \n",
    "  # Storing every calculated loss and accuracy to calculate the average\n",
    "  lossHistory = []\n",
    "  accuracyHistory = []\n",
    "  \n",
    "  with torch.no_grad(): # Disable gradient computation\n",
    "    for test_data, test_targ in test_loader:\n",
    "      y_test = network(test_data) # Perform forward pass\n",
    "      test_loss = lossFunction(y_test, test_targ) # Compute the loss\n",
    "      test_accuracy = torch.mean((torch.argmax(y_test,dim=1) == test_targ).float()) # Compute the accuracy\n",
    "\n",
    "      # Add loss and accuracy to their lists to calculate the average later.\n",
    "      lossHistory.append(test_loss.detach().item())\n",
    "      accuracyHistory.append(test_accuracy.detach())\n",
    "\n",
    "    # Calculate the average loss and accuracy for the test set.\n",
    "    avg_loss = sum(lossHistory) / len(lossHistory)\n",
    "    avg_accuracy = sum(accuracyHistory) / len(accuracyHistory)\n",
    "\n",
    "    # Print the test accuracy\n",
    "    print(f\"Test Accuracy: {int(torch.round(avg_accuracy * 100).item())}%\")\n",
    "\n",
    "    # Choose five examples with their predictions and actual labels\n",
    "\n",
    "    # Extracting the predictions of the first five test samples\n",
    "    example_predictions = torch.argmax(y_test, dim=1)[:5]\n",
    "    example_labels = test_targ[:5]\n",
    "\n",
    "    # Print the predictions vs actual labels for those five samples\n",
    "    for i in range(5):\n",
    "        print(f\"Test Image {i+1}: Predicted Label = {example_predictions[i].item()}, Actual Label = {example_labels[i].item()}\")\n",
    "\n",
    "test_network(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Activation Function Experiments\n",
    "Additional experiments with different activation functions (ReLU, Sigmoid, Tanh) to assess their impact on training speed and final test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1716933290751,
     "user": {
      "displayName": "Cheryl C",
      "userId": "07083983319276283397"
     },
     "user_tz": -720
    },
    "id": "OwqedsLgCQbn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ReLU activation function:\n",
      "Model Training:\n",
      "Epoch 1: Loss = 2.288, Accuracy = 41%\n",
      "Epoch 2: Loss = 2.166, Accuracy = 59%\n",
      "Epoch 3: Loss = 1.888, Accuracy = 66%\n",
      "Epoch 4: Loss = 1.733, Accuracy = 80%\n",
      "Epoch 5: Loss = 1.664, Accuracy = 84%\n",
      "Epoch 6: Loss = 1.631, Accuracy = 86%\n",
      "Epoch 7: Loss = 1.580, Accuracy = 93%\n",
      "Epoch 8: Loss = 1.559, Accuracy = 94%\n",
      "Epoch 9: Loss = 1.544, Accuracy = 94%\n",
      "Epoch 10: Loss = 1.531, Accuracy = 95%\n",
      "Epoch 11: Loss = 1.522, Accuracy = 96%\n",
      "Epoch 12: Loss = 1.518, Accuracy = 96%\n",
      "Epoch 13: Loss = 1.510, Accuracy = 97%\n",
      "Epoch 14: Loss = 1.512, Accuracy = 96%\n",
      "Epoch 15: Loss = 1.502, Accuracy = 97%\n",
      "\n",
      "Model Evaluation:\n",
      "Test Accuracy: 95%\n",
      "Test Image 1: Predicted Label = 5, Actual Label = 5\n",
      "Test Image 2: Predicted Label = 3, Actual Label = 3\n",
      "Test Image 3: Predicted Label = 1, Actual Label = 1\n",
      "Test Image 4: Predicted Label = 2, Actual Label = 2\n",
      "Test Image 5: Predicted Label = 9, Actual Label = 9\n",
      "\n",
      "Sigmoid activation function:\n",
      "Model Training:\n",
      "Epoch 1: Loss = 2.304, Accuracy = 9%\n",
      "Epoch 2: Loss = 2.300, Accuracy = 11%\n",
      "Epoch 3: Loss = 2.297, Accuracy = 11%\n",
      "Epoch 4: Loss = 2.290, Accuracy = 11%\n",
      "Epoch 5: Loss = 2.269, Accuracy = 15%\n",
      "Epoch 6: Loss = 2.224, Accuracy = 26%\n",
      "Epoch 7: Loss = 2.171, Accuracy = 35%\n",
      "Epoch 8: Loss = 2.106, Accuracy = 39%\n",
      "Epoch 9: Loss = 2.029, Accuracy = 57%\n",
      "Epoch 10: Loss = 1.955, Accuracy = 62%\n",
      "Epoch 11: Loss = 1.902, Accuracy = 64%\n",
      "Epoch 12: Loss = 1.869, Accuracy = 65%\n",
      "Epoch 13: Loss = 1.847, Accuracy = 65%\n",
      "Epoch 14: Loss = 1.835, Accuracy = 65%\n",
      "Epoch 15: Loss = 1.827, Accuracy = 66%\n",
      "\n",
      "Model Evaluation:\n",
      "Test Accuracy: 63%\n",
      "Test Image 1: Predicted Label = 9, Actual Label = 5\n",
      "Test Image 2: Predicted Label = 9, Actual Label = 3\n",
      "Test Image 3: Predicted Label = 1, Actual Label = 1\n",
      "Test Image 4: Predicted Label = 2, Actual Label = 2\n",
      "Test Image 5: Predicted Label = 9, Actual Label = 9\n",
      "\n",
      "Tanh activation function:\n",
      "Model Training:\n",
      "Epoch 1: Loss = 2.266, Accuracy = 34%\n",
      "Epoch 2: Loss = 2.051, Accuracy = 68%\n",
      "Epoch 3: Loss = 1.782, Accuracy = 80%\n",
      "Epoch 4: Loss = 1.659, Accuracy = 89%\n",
      "Epoch 5: Loss = 1.592, Accuracy = 92%\n",
      "Epoch 6: Loss = 1.560, Accuracy = 94%\n",
      "Epoch 7: Loss = 1.539, Accuracy = 95%\n",
      "Epoch 8: Loss = 1.528, Accuracy = 95%\n",
      "Epoch 9: Loss = 1.520, Accuracy = 96%\n",
      "Epoch 10: Loss = 1.513, Accuracy = 97%\n",
      "Epoch 11: Loss = 1.511, Accuracy = 96%\n",
      "Epoch 12: Loss = 1.505, Accuracy = 97%\n",
      "Epoch 13: Loss = 1.500, Accuracy = 98%\n",
      "Epoch 14: Loss = 1.497, Accuracy = 97%\n",
      "Epoch 15: Loss = 1.494, Accuracy = 98%\n",
      "\n",
      "Model Evaluation:\n",
      "Test Accuracy: 96%\n",
      "Test Image 1: Predicted Label = 5, Actual Label = 5\n",
      "Test Image 2: Predicted Label = 3, Actual Label = 3\n",
      "Test Image 3: Predicted Label = 1, Actual Label = 1\n",
      "Test Image 4: Predicted Label = 2, Actual Label = 2\n",
      "Test Image 5: Predicted Label = 9, Actual Label = 9\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with Activation Functions: Comparing the effectiveness of ReLU, Sigmoid, and Tanh.\n",
    "\n",
    "# Modifying NeuralNetwork() to have __init__ accept an activation function as a parameter\n",
    "# Referring to this class as NewNeuralNetwork()\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NewNeuralNetwork(torch.nn.Module):\n",
    "  def __init__(self, activation_function):\n",
    "    super().__init__()\n",
    "    # Define the three layers using torch.nn.Linear (creates fully connected/linear layers)\n",
    "    # Input layer has 64 neurons (64 features) and transforms that input data into 128 neurons\n",
    "    self.input_layer = torch.nn.Linear(64, 128)\n",
    "    # Hidden layer with 128 neurons in and out\n",
    "    self.hidden_layer = torch.nn.Linear(128, 128)\n",
    "    # Output layer with 10 neurons out that correspond to the 10 classes (digits 0-9)\n",
    "    self.output_layer = torch.nn.Linear(128, 10)\n",
    "    self.activation_function = activation_function\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Forward pass through the neural network\n",
    "    x = self.activation_function(self.input_layer(x))\n",
    "    x = self.activation_function(self.hidden_layer(x))\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    x = softmax(self.output_layer(x))\n",
    "    return x\n",
    "\n",
    "\"\"\"\n",
    "Activation functions introduce non-linearity into the model.\n",
    "\n",
    "ReLU: (max(z, 0)), replaces any negative values with 0\n",
    "and preserves positive values; activates only the neurons\n",
    "with positive outputs to make the model sparse and efficient.\n",
    "\n",
    "Sigmoid: Maps any input z to a value between 0 and 1, which can be\n",
    "interpreted as probabilities.\n",
    "\n",
    "Tanh: Maps any input z to a value between -1 and 1, makes it zero-centered\n",
    "which can help center the data and make optimisation easier.\n",
    "\n",
    "ReLU and Tanh often used in hidden layers, Sigmoid in output layers for binary\n",
    "classfication.\n",
    "\"\"\"\n",
    "activation_functions = {\n",
    "    'ReLU': F.relu,\n",
    "    'Sigmoid': torch.sigmoid,\n",
    "    'Tanh': torch.tanh\n",
    "}\n",
    "\n",
    "# Train and test the network with each activation function\n",
    "for name, act_fnc in activation_functions.items():\n",
    "    print(f\"\\n{name} activation function:\")\n",
    "    # Initialising neural network using NewNeuralNetwork class\n",
    "    new_model = NewNeuralNetwork(act_fnc)\n",
    "    # Retraining same optimiser and learning rate as before\n",
    "    new_optimiser = torch.optim.Adam(new_model.parameters(), lr=0.001)\n",
    "    # Retraining same loss function as before\n",
    "    new_lossFunc = torch.nn.CrossEntropyLoss()\n",
    "    # train_show will print all 15 epoch results\n",
    "    print(\"Model Training:\")\n",
    "    train_show(new_model, new_lossFunc, new_optimiser, 15)\n",
    "    # test_network will print average accuracy and five sample results\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    test_network(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Activation Function Comparison\n",
    "\n",
    "- *ReLU* had fast convergence, stable training, and a strong final test accuracy of 95%.\n",
    "- *Sigmoid* had slow learning and struggled to propagate gradients early in training, with its final test accuracy being only 63%.\n",
    "- *Tanh* had slightly better convergence and final test accuracy (96%) than ReLU.\n",
    "\n",
    "The conclusion is that **ReLU and Tanh are the most effective** for this MLP on the Digits dataset, while Sigmoid underperforms due to vanishing gradients."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMmCVYFBBUtEU+P+95QVxsJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
